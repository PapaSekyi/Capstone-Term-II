{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "514e44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import pint\n",
    "ureg = pint.UnitRegistry()\n",
    "# IN the is we get all the symbols associated with the data \n",
    "import pickle \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding,Dropout,Activation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from goose3 import Goose\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f5d215d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    while True:\n",
    "        global url\n",
    "        url=input(\"Enter the word to be searched on wikipedia \\n\") \n",
    "        url1 =f'https://en.wikipedia.org/wiki/{url}'\n",
    "        try:\n",
    "            text=Goose().extract(url1).cleaned_text\n",
    "            return text,url \n",
    "        except:\n",
    "            print(\" Failed To get Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fecee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    text,url=get_data()\n",
    "    text=text.replace('\\n','')\n",
    "    text=text.lower() \n",
    "    text=sent_tokenize(text)\n",
    "    save_text = text \n",
    "    translator = str.maketrans('','',string.punctuation + '0123456789'+'•°'+'─')\n",
    "    clean_text=[]\n",
    "    for i in text:\n",
    "        a=i.translate(translator)\n",
    "        clean_text.append(a) \n",
    "    cleaner_text=[]\n",
    "    for i in clean_text:\n",
    "        l=[]\n",
    "        for j in i.split():\n",
    "            if j not in stopwords+list(ureg):\n",
    "                l.append(j)\n",
    "        cleaner_text.append(' '.join(l)) \n",
    "    text = ' '.join(cleaner_text)    \n",
    "    tokens=text.split()\n",
    "    print(f\"Total data that we are taking in consideration is {len(tokens)}\\n\") \n",
    "    print(f\" Unique words in our data is {len(set(tokens))}\\n\")\n",
    "    # Need to save the tokenizer here \n",
    "    length = 4\n",
    "    lines=[]\n",
    "    for i in range(length,len(tokens)):\n",
    "        seq=tokens[i-length:i]\n",
    "        line = ' '.join(seq)\n",
    "        lines.append(line)\n",
    "    predicted_word=[] \n",
    "    for i in range(len(tokens)-4):\n",
    "        predicted_word.append(tokens[i+4])\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences= np.array(tokenizer.texts_to_sequences(lines))\n",
    "    output_sequences=tokenizer.texts_to_sequences(predicted_word)\n",
    "    X = np.zeros((len(sequences)-2,4,len(set(tokens))),dtype='bool')\n",
    "    y= np.zeros((len(output_sequences)-2,len(set(tokens))),dtype='bool')\n",
    "    sequences=sequences[:len(sequences)-10]\n",
    "    output_sequences=output_sequences[:len(output_sequences)-10]\n",
    "    for i,words in enumerate(sequences):# Here basically i gives you the index and the words are a set of 1st column that we see.\n",
    "    # to the J we are only passing 4 word sequnece, \n",
    "        for j,word in enumerate(words):\n",
    "        # there are going to be 1309 j values\n",
    "            X[i,j,word] =1  # just giving value of 1 to each word that we have\n",
    "        y[i,output_sequences[i][0]]=1\n",
    "# SO here basically we have mapped out each word in an array of 750 words to have a value of 1.\n",
    "# Save the tokenizer to a file using pickle\n",
    "    with open('tokenizer.pickle', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    return X,y,len(set(tokens)),url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35bcaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X,y,size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128,input_shape=(4,size),return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=RMSprop(learning_rate=0.01),metrics=['accuracy'])\n",
    "    model.fit(X,y,batch_size=128,epochs=40,shuffle=True,verbose=0) \n",
    "    model.save(f\"{url}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3f1878c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem HEre is that if there doesn't exist a word in the training set of our data, then HOw do we handle that.\n",
    "def predict_next_word(input_text):# best n predictions\n",
    "    m=[]\n",
    "    X,y,size,url=preprocess_data()\n",
    "    with open('tokenizer.pickle', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)  \n",
    "    try:\n",
    "        model = load_model(f\"{url}.h5\") \n",
    "    except:\n",
    "        run_model(X,y,size)\n",
    "        model = load_model(f\"{url}.h5\") \n",
    "    n={value: key for key, value in tokenizer.word_index.items()}\n",
    "    input_text=input_text.lower()\n",
    "    input_sequence=tokenizer.texts_to_sequences([input_text])[0]\n",
    "    X=np.zeros((1,4,size))  # 1 set with 4 words each in a vector representation of 750 words\n",
    "    # we only have set of one array\n",
    "    for i,word in enumerate(input_sequence):\n",
    "        X[0,i,word]=1\n",
    "    predict=model.predict(X)[0]\n",
    "    print(\"The best possible 4 words are: -\\n\") \n",
    "    for i in np.argsort(predict)[-4:][::-1]:\n",
    "        print(n[i])\n",
    "        m.append(n[i])\n",
    "    return m \n",
    "    \n",
    "    \n",
    "#     return l  \n",
    "#     if len(input_sequence[0]) < 4:\n",
    "#         # we need more words, so I am going to save these in an array as of now and if in the next prompt \n",
    "#         # I have more than 4 words then I am going to use the last best 4 words\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c00af8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word to be searched on wikipedia \n",
      "Game\n",
      "Total data that we are taking in consideration is 2591\n",
      "\n",
      " Unique words in our data is 1206\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "The best possible 4 words are: -\n",
      "\n",
      "may\n",
      "quantum\n",
      "take\n",
      "player\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['may', 'quantum', 'take', 'player']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word(\"Game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "403c16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e59ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
